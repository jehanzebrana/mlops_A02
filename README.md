# ğŸ“Š MLOPS_Assignment02: Data Preprocessing Workflow

## ğŸ“„ Overview
This repository presents a Python script and an Apache Airflow DAG designed to automate the extraction and preprocessing of data from the BBC News website. Below is an outline of the workflow and the hurdles encountered during implementation.

## ğŸ› ï¸ Workflow Summary

### ğŸ“° Data Extraction
The script utilizes BeautifulSoup and requests libraries to scrape headlines and descriptions from the BBC News homepage.

### âœ¨ Text Preprocessing
Text preprocessing is executed through a `preprocess_text` function, which cleans and formats the extracted text data.

### ğŸ’¾ Data Storage and Version Control
Preprocessed data is stored in a CSV file, and Data Version Control (DVC) is implemented to track versions and ensure reproducibility.

### ğŸš€ Apache Airflow DAG Development
An Airflow DAG named `assignment_dag` is created to automate the data extraction, preprocessing, push to DVC and GitHub.

## ğŸš§ Challenges Faced
- **Apache Airflow Installation:** Difficulties were encountered during Apache Airflow installation due to compatibility issues and system dependencies.
- **Automating Git Operations:** Automating git operations within Airflow tasks without manual intervention.
- **Handling URLs:** Dealing with relative URLs and converting them to absolute.

## ğŸ Conclusion
Combining Apache Airflow, DVC, and GitHub creates a powerful system for orchestrating data pipelines. This setup guarantees efficient, dependable, and transparent processes for data extraction, transformation, and loading. The project underscores the critical roles of data quality, automation, and version control in contemporary data engineering.
